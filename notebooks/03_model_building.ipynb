{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building for Disease Prediction\n",
    "\n",
    "This notebook focuses on building and comparing different models for predicting disease outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from modeling import train_test_validation_split, evaluate_classification_model, plot_confusion_matrix, plot_roc_curve\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the path to your actual processed data file\n",
    "# data_path = '../data/processed_features.csv'\n",
    "# df = pd.read_csv(data_path)\n",
    "\n",
    "# For now, create a synthetic dataset\n",
    "n_samples = 500\n",
    "n_features = 15\n",
    "\n",
    "# Generate synthetic data with some predictive signal\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "# Make the first few features more predictive\n",
    "true_coef = np.zeros(n_features)\n",
    "true_coef[:5] = np.array([2.0, 1.0, 0.8, -1.5, 1.2])\n",
    "y_prob = 1 / (1 + np.exp(-np.dot(X, true_coef) + np.random.randn(n_samples) * 0.5))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Create a dataframe\n",
    "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['disease_status'] = y\n",
    "\n",
    "# Show the data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Positive cases: {sum(y)} ({sum(y)/len(y):.1%})\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['disease_status'])\n",
    "y = df['disease_status']\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_validation_split(\n",
    "    X, y, test_size=0.2, validation_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a logistic regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = lr_model.predict(X_val)\n",
    "y_val_prob = lr_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Print basic metrics\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_auc_score(y_val, y_val_prob):.4f}\")\n",
    "print(f\"Validation F1 score: {f1_score(y_val, y_val_pred):.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_val, y_val_prob, label='Logistic Regression')\n",
    "\n",
    "# Show feature coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a random forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Print basic metrics\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred_rf):.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_auc_score(y_val, y_val_prob_rf):.4f}\")\n",
    "print(f\"Validation F1 score: {f1_score(y_val, y_val_pred_rf):.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(confusion_matrix(y_val, y_val_pred_rf), class_names=['Negative', 'Positive'])\n",
    "\n",
    "# Show feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10))\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train a gradient boosting model\n",
    "print(\"Training Gradient Boosting model...\")\n",
    "start_time = time.time()\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_gb = gb_model.predict(X_val)\n",
    "y_val_prob_gb = gb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Print basic metrics\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred_gb):.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_auc_score(y_val, y_val_prob_gb):.4f}\")\n",
    "print(f\"Validation F1 score: {f1_score(y_val, y_val_pred_gb):.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_val, y_val_prob)\n",
    "roc_auc_lr = roc_auc_score(y_val, y_val_prob)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.3f})')\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_val, y_val_prob_rf)\n",
    "roc_auc_rf = roc_auc_score(y_val, y_val_prob_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n",
    "\n",
    "# Gradient Boosting\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_val, y_val_prob_gb)\n",
    "roc_auc_gb = roc_auc_score(y_val, y_val_prob_gb)\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {roc_auc_gb:.3f})')\n",
    "\n",
    "# Random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table\n",
    "models = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']\n",
    "y_val_probs = [y_val_prob, y_val_prob_rf, y_val_prob_gb]\n",
    "y_val_preds = [y_val_pred, y_val_pred_rf, y_val_pred_gb]\n",
    "\n",
    "results = []\n",
    "for model, y_prob, y_pred in zip(models, y_val_probs, y_val_preds):\n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_val, y_prob),\n",
    "        'Precision': precision_score(y_val, y_pred),\n",
    "        'Recall': recall_score(y_val, y_pred),\n",
    "        'F1 Score': f1_score(y_val, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Best Model\n",
    "\n",
    "Based on the above comparison, let's tune the hyperparameters for the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Gradient Boosting is the best model (you should replace with your best model)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# For demonstration, use a smaller grid\n",
    "param_grid_small = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "print(\"Running grid search (this may take a while)...\")\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_small,  # Using smaller grid for demonstration\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Create the best model with tuned parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_val_pred = best_model.predict(X_val)\n",
    "best_val_prob = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"\\nTuned model validation AUC: {roc_auc_score(y_val, best_val_prob):.4f}\")\n",
    "print(f\"Tuned model validation F1: {f1_score(y_val, best_val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "Now that we've selected and tuned our best model, let's evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Full evaluation\n",
    "test_metrics = evaluate_classification_model(best_model, X_test, y_test)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Test PR AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(test_metrics['classification_report'])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_metrics['confusion_matrix'], class_names=['Negative', 'Positive'])\n",
    "\n",
    "# Plot final ROC curve\n",
    "plot_roc_curve(y_test, y_test_prob, label='Final Tuned Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to disk\n",
    "model_path = '../models/disease_prediction_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names_path = '../models/feature_names.pkl'\n",
    "joblib.dump(X.columns.tolist(), feature_names_path)\n",
    "print(f\"Feature names saved to {feature_names_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "Let's try to interpret what features are most important for our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the best model has feature_importances_ attribute\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models like Random Forest or Gradient Boosting\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models like Logistic Regression\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_[0]\n",
    "    }).sort_values('Coefficient', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef_df.head(10))\n",
    "    plt.title('Top 10 Feature Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"Top 10 features by coefficient magnitude:\")\n",
    "    print(coef_df.head(10))\n",
    "else:\n",
    "    print(\"Feature importance not directly available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we have:\n",
    "- Built and compared several classification models for disease prediction\n",
    "- Tuned the hyperparameters of the best-performing model\n",
    "- Evaluated the final model on the test set\n",
    "- Saved the model for future use\n",
    "- Interpreted the most important features for prediction\n",
    "\n",
    "Next steps:\n",
    "1. Apply the model to real patient data\n",
    "2. Develop a simple API or interface for model predictions\n",
    "3. Monitor model performance over time\n",
    "4. Consider exploring more advanced models or ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}