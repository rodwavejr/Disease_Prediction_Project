{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-IV Data Exploration\n",
    "\n",
    "This notebook explores the MIMIC-IV dataset for integration into our COVID-19 detection pipeline. We'll examine the structure of the dataset, identify relevant tables, and prepare them for use in our NER and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-IV Dataset Overview\n",
    "\n",
    "MIMIC-IV is a large, freely available clinical database containing de-identified data from hospital admissions. The dataset includes:\n",
    "\n",
    "1. **Hospital Data (hosp)**: Contains administrative data, diagnoses, procedures, medications, laboratory tests\n",
    "2. **ICU Data (icu)**: Contains detailed ICU information including vitals, clinical measurements, and interventions\n",
    "\n",
    "For our COVID-19 detection project, we're interested in clinical notes, symptoms, diagnoses, and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to MIMIC-IV data\n",
    "MIMIC_PATH = '/Users/Apexr/physionet.org/files/mimiciv/3.1'\n",
    "HOSP_PATH = os.path.join(MIMIC_PATH, 'hosp')\n",
    "ICU_PATH = os.path.join(MIMIC_PATH, 'icu')\n",
    "\n",
    "# Define project data directory\n",
    "PROJECT_DATA_DIR = '../data'\n",
    "PROJECT_MIMIC_DIR = os.path.join(PROJECT_DATA_DIR, 'external/mimic')\n",
    "\n",
    "# Create directory for organized MIMIC data in our project\n",
    "os.makedirs(PROJECT_MIMIC_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data Dictionary\n",
    "\n",
    "Let's first examine the patients, admissions, and diagnoses tables to understand their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gzip_csv(filepath, nrows=None, sample_frac=None):\n",
    "    \"\"\"Read a gzipped CSV file with options for sampling rows\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, nrows=nrows, compression='gzip')\n",
    "        \n",
    "        if sample_frac is not None and nrows is None:\n",
    "            df = df.sample(frac=sample_frac, random_state=42)\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read patients data (sample)\n",
    "patients_path = os.path.join(HOSP_PATH, 'patients.csv.gz')\n",
    "patients_df = read_gzip_csv(patients_path, nrows=5)\n",
    "print(f\"Patients table structure:\")\n",
    "display(patients_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read admissions data (sample)\n",
    "admissions_path = os.path.join(HOSP_PATH, 'admissions.csv.gz')\n",
    "admissions_df = read_gzip_csv(admissions_path, nrows=5)\n",
    "print(f\"Admissions table structure:\")\n",
    "display(admissions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read diagnoses dictionary\n",
    "d_icd_diagnoses_path = os.path.join(HOSP_PATH, 'd_icd_diagnoses.csv.gz')\n",
    "d_icd_diagnoses_df = read_gzip_csv(d_icd_diagnoses_path)\n",
    "\n",
    "# Look for COVID-19 related diagnoses\n",
    "covid_diagnoses = d_icd_diagnoses_df[d_icd_diagnoses_df['long_title'].str.contains('COVID|coronavirus|SARS', case=False, na=False)]\n",
    "print(f\"COVID-19 related diagnoses:\")\n",
    "display(covid_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read diagnoses data (sample)\n",
    "diagnoses_path = os.path.join(HOSP_PATH, 'diagnoses_icd.csv.gz')\n",
    "diagnoses_df = read_gzip_csv(diagnoses_path, nrows=100)\n",
    "print(f\"Diagnoses table structure:\")\n",
    "display(diagnoses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Clinical Notes\n",
    "\n",
    "For our NER component, we need clinical notes. Let's check if the MIMIC-IV dataset contains clinical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for a notes file in the hospital data\n",
    "import glob\n",
    "\n",
    "# List all files in the hospital directory\n",
    "hospital_files = os.listdir(HOSP_PATH)\n",
    "note_files = [f for f in hospital_files if 'note' in f.lower()]\n",
    "print(f\"Files containing 'note' in hosp directory: {note_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Missing Note Events\n",
    "\n",
    "MIMIC-IV doesn't seem to contain clinical notes directly in its 3.1 version. The full clinical notes are typically in a separate module called MIMIC-IV-Note that may need separate access.\n",
    "\n",
    "For our project, we can:\n",
    "1. Use the OMR (Outpatient Medication Reconciliation) table which contains some text data\n",
    "2. Use the MIMIC-III notes that may be available elsewhere\n",
    "3. Continue with our approach of using synthetic data as well as other real data sources like clinical trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the OMR (Outpatient Medication Reconciliation) table for text content\n",
    "omr_path = os.path.join(HOSP_PATH, 'omr.csv.gz')\n",
    "if os.path.exists(omr_path):\n",
    "    omr_df = read_gzip_csv(omr_path, nrows=5)\n",
    "    print(f\"OMR table structure:\")\n",
    "    display(omr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data for our COVID-19 Detection Project\n",
    "\n",
    "Let's extract the most relevant data for our project:\n",
    "\n",
    "1. Patient demographics\n",
    "2. COVID-19 diagnoses\n",
    "3. Any available text data that could be used for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract patient demographic data\n",
    "patients_sample = read_gzip_csv(patients_path, sample_frac=0.1)\n",
    "if patients_sample is not None:\n",
    "    # Save to project directory\n",
    "    patients_output_path = os.path.join(PROJECT_MIMIC_DIR, 'patients_sample.csv')\n",
    "    patients_sample.to_csv(patients_output_path, index=False)\n",
    "    print(f\"Saved patient sample ({len(patients_sample)} records) to {patients_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract admissions data\n",
    "admissions_sample = read_gzip_csv(admissions_path, sample_frac=0.1)\n",
    "if admissions_sample is not None:\n",
    "    # Save to project directory\n",
    "    admissions_output_path = os.path.join(PROJECT_MIMIC_DIR, 'admissions_sample.csv')\n",
    "    admissions_sample.to_csv(admissions_output_path, index=False)\n",
    "    print(f\"Saved admissions sample ({len(admissions_sample)} records) to {admissions_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract diagnoses and search for possible COVID-19 cases\n",
    "# We need both the diagnoses and the dictionary\n",
    "d_icd_output_path = os.path.join(PROJECT_MIMIC_DIR, 'd_icd_diagnoses.csv')\n",
    "d_icd_diagnoses_df.to_csv(d_icd_output_path, index=False)\n",
    "print(f\"Saved diagnosis dictionary to {d_icd_output_path}\")\n",
    "\n",
    "# Find relevant ICD codes for COVID-19 and respiratory conditions\n",
    "covid_icd_codes = covid_diagnoses['icd_code'].tolist()\n",
    "print(f\"Found {len(covid_icd_codes)} COVID-19 related ICD codes\")\n",
    "\n",
    "# Also include other respiratory conditions that might be COVID-related\n",
    "resp_diagnoses = d_icd_diagnoses_df[d_icd_diagnoses_df['long_title'].str.contains('pneumonia|respiratory|breathing', case=False, na=False)]\n",
    "resp_icd_codes = resp_diagnoses['icd_code'].tolist()\n",
    "print(f\"Found {len(resp_icd_codes)} respiratory condition ICD codes\")\n",
    "\n",
    "# Combine all relevant codes\n",
    "relevant_icd_codes = covid_icd_codes + resp_icd_codes\n",
    "print(f\"Total of {len(relevant_icd_codes)} relevant ICD codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read diagnoses data in batches and extract relevant cases\n",
    "def extract_relevant_diagnoses(diagnoses_path, icd_codes, output_path, chunk_size=100000):\n",
    "    \"\"\"Extract diagnoses matching the specified ICD codes\"\"\"\n",
    "    relevant_diagnoses = []\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks to handle large file size\n",
    "        for chunk in pd.read_csv(diagnoses_path, chunksize=chunk_size, compression='gzip'):\n",
    "            # Filter to relevant diagnoses\n",
    "            mask = chunk['icd_code'].isin(icd_codes)\n",
    "            if mask.any():\n",
    "                relevant_chunk = chunk[mask]\n",
    "                relevant_diagnoses.append(relevant_chunk)\n",
    "                print(f\"Found {len(relevant_chunk)} relevant diagnoses in this chunk\")\n",
    "                \n",
    "        if relevant_diagnoses:\n",
    "            # Combine all relevant diagnoses\n",
    "            result_df = pd.concat(relevant_diagnoses, ignore_index=True)\n",
    "            # Save to file\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved {len(result_df)} relevant diagnoses to {output_path}\")\n",
    "            return result_df\n",
    "        else:\n",
    "            print(\"No relevant diagnoses found\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing diagnoses: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Extract relevant diagnoses\n",
    "diagnoses_output_path = os.path.join(PROJECT_MIMIC_DIR, 'relevant_diagnoses.csv')\n",
    "relevant_diagnoses_df = extract_relevant_diagnoses(diagnoses_path, relevant_icd_codes, diagnoses_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract OMR data for text analysis\n",
    "omr_sample_path = os.path.join(PROJECT_MIMIC_DIR, 'omr_sample.csv')\n",
    "\n",
    "try:\n",
    "    # Process file in chunks to handle large file size\n",
    "    chunk_size = 10000\n",
    "    sample_size = 1000\n",
    "    \n",
    "    # Initialize an empty list to store sampled chunks\n",
    "    sampled_chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(omr_path, chunksize=chunk_size, compression='gzip'):\n",
    "        # Sample each chunk\n",
    "        sample_fraction = min(sample_size / chunk.shape[0], 1.0)\n",
    "        sampled_chunk = chunk.sample(frac=sample_fraction, random_state=42)\n",
    "        sampled_chunks.append(sampled_chunk)\n",
    "        \n",
    "        total_rows += len(sampled_chunk)\n",
    "        if total_rows >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Combine all sampled chunks\n",
    "    omr_sample = pd.concat(sampled_chunks, ignore_index=True)\n",
    "    # Limit to sample_size if we got more\n",
    "    if len(omr_sample) > sample_size:\n",
    "        omr_sample = omr_sample.head(sample_size)\n",
    "        \n",
    "    # Save to file\n",
    "    omr_sample.to_csv(omr_sample_path, index=False)\n",
    "    print(f\"Saved {len(omr_sample)} OMR records to {omr_sample_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing OMR data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Lab Results Related to COVID-19\n",
    "\n",
    "Let's extract lab test results that might be relevant to COVID-19 patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lab items dictionary for COVID-19 tests\n",
    "labitems_path = os.path.join(HOSP_PATH, 'd_labitems.csv.gz')\n",
    "labitems_df = read_gzip_csv(labitems_path)\n",
    "\n",
    "# Look for COVID-19 related tests\n",
    "covid_lab_items = labitems_df[labitems_df['label'].str.contains('COVID|coronavirus|SARS|antibody', case=False, na=False)]\n",
    "print(f\"COVID-19 related lab tests:\")\n",
    "display(covid_lab_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lab items dictionary\n",
    "labitems_output_path = os.path.join(PROJECT_MIMIC_DIR, 'd_labitems.csv')\n",
    "labitems_df.to_csv(labitems_output_path, index=False)\n",
    "print(f\"Saved lab items dictionary to {labitems_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lab events for the patients with relevant diagnoses if we have them\n",
    "if not relevant_diagnoses_df.empty:\n",
    "    # Get unique subject_ids from relevant diagnoses\n",
    "    relevant_subject_ids = relevant_diagnoses_df['subject_id'].unique().tolist()\n",
    "    print(f\"Found {len(relevant_subject_ids)} unique patients with relevant diagnoses\")\n",
    "    \n",
    "    # Extract lab events for these patients\n",
    "    labevents_path = os.path.join(HOSP_PATH, 'labevents.csv.gz')\n",
    "    labevents_output_path = os.path.join(PROJECT_MIMIC_DIR, 'relevant_labevents.csv')\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks to handle large file size\n",
    "        relevant_labevents = []\n",
    "        total_relevant = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(labevents_path, chunksize=100000, compression='gzip'):\n",
    "            # Filter to relevant subject_ids\n",
    "            mask = chunk['subject_id'].isin(relevant_subject_ids)\n",
    "            if mask.any():\n",
    "                relevant_chunk = chunk[mask]\n",
    "                relevant_labevents.append(relevant_chunk)\n",
    "                total_relevant += len(relevant_chunk)\n",
    "                print(f\"Found {len(relevant_chunk)} relevant lab events in this chunk. Total so far: {total_relevant}\")\n",
    "                \n",
    "        if relevant_labevents:\n",
    "            # Combine all relevant lab events\n",
    "            result_df = pd.concat(relevant_labevents, ignore_index=True)\n",
    "            # Save to file\n",
    "            result_df.to_csv(labevents_output_path, index=False)\n",
    "            print(f\"Saved {len(result_df)} relevant lab events to {labevents_output_path}\")\n",
    "        else:\n",
    "            print(\"No relevant lab events found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lab events: {e}\")\n",
    "else:\n",
    "    print(\"No relevant diagnoses to match with lab events. Taking a sample instead.\")\n",
    "    # Take a random sample of lab events\n",
    "    labevents_path = os.path.join(HOSP_PATH, 'labevents.csv.gz')\n",
    "    labevents_sample_path = os.path.join(PROJECT_MIMIC_DIR, 'labevents_sample.csv')\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks to handle large file size\n",
    "        chunk_size = 100000\n",
    "        sample_size = 1000\n",
    "        \n",
    "        # Initialize an empty list to store sampled chunks\n",
    "        sampled_chunks = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(labevents_path, chunksize=chunk_size, compression='gzip'):\n",
    "            # Sample each chunk\n",
    "            sample_fraction = min(sample_size / chunk.shape[0], 1.0)\n",
    "            sampled_chunk = chunk.sample(frac=sample_fraction, random_state=42)\n",
    "            sampled_chunks.append(sampled_chunk)\n",
    "            \n",
    "            total_rows += len(sampled_chunk)\n",
    "            if total_rows >= sample_size:\n",
    "                break\n",
    "        \n",
    "        # Combine all sampled chunks\n",
    "        labevents_sample = pd.concat(sampled_chunks, ignore_index=True)\n",
    "        # Limit to sample_size if we got more\n",
    "        if len(labevents_sample) > sample_size:\n",
    "            labevents_sample = labevents_sample.head(sample_size)\n",
    "            \n",
    "        # Save to file\n",
    "        labevents_sample.to_csv(labevents_sample_path, index=False)\n",
    "        print(f\"Saved {len(labevents_sample)} lab events to {labevents_sample_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing lab events sample: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Data Extraction\n",
    "\n",
    "Let's also extract some chart events from the ICU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a sample of chart events\n",
    "chartevents_path = os.path.join(ICU_PATH, 'chartevents.csv.gz')\n",
    "chartevents_sample_path = os.path.join(PROJECT_MIMIC_DIR, 'chartevents_sample.csv')\n",
    "\n",
    "try:\n",
    "    # Process file in chunks to handle large file size\n",
    "    chunk_size = 100000\n",
    "    sample_size = 1000\n",
    "    \n",
    "    # Initialize an empty list to store sampled chunks\n",
    "    sampled_chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(chartevents_path, chunksize=chunk_size, compression='gzip'):\n",
    "        # Sample each chunk\n",
    "        sample_fraction = min(sample_size / chunk.shape[0], 1.0)\n",
    "        sampled_chunk = chunk.sample(frac=sample_fraction, random_state=42)\n",
    "        sampled_chunks.append(sampled_chunk)\n",
    "        \n",
    "        total_rows += len(sampled_chunk)\n",
    "        if total_rows >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Combine all sampled chunks\n",
    "    chartevents_sample = pd.concat(sampled_chunks, ignore_index=True)\n",
    "    # Limit to sample_size if we got more\n",
    "    if len(chartevents_sample) > sample_size:\n",
    "        chartevents_sample = chartevents_sample.head(sample_size)\n",
    "        \n",
    "    # Save to file\n",
    "    chartevents_sample.to_csv(chartevents_sample_path, index=False)\n",
    "    print(f\"Saved {len(chartevents_sample)} chart events to {chartevents_sample_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing chart events: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with Our Pipeline\n",
    "\n",
    "Now let's create a function to load this data into our pipeline for the COVID-19 detection project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mimic_integration_module():\n",
    "    \"\"\"Create a Python module to integrate MIMIC data with our pipeline\"\"\"\n",
    "    module_path = '../src/mimic_integration.py'\n",
    "    \n",
    "    code = \"\"\"\\\n",
    "\"\"\"MIMIC-IV Integration Module\n",
    "\n",
    "This module provides functions to integrate MIMIC-IV data into our COVID-19 detection pipeline.\n",
    "It handles loading and preprocessing the data for both NER and classification tasks.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Path to the MIMIC data in our project\n",
    "MIMIC_DIR = os.path.join('data', 'external', 'mimic')\n",
    "\n",
    "def load_mimic_demographics():\n",
    "    \"\"\"Load patient demographic data from MIMIC-IV\"\"\"\n",
    "    patients_path = os.path.join(MIMIC_DIR, 'patients_sample.csv')\n",
    "    admissions_path = os.path.join(MIMIC_DIR, 'admissions_sample.csv')\n",
    "    \n",
    "    try:\n",
    "        patients_df = pd.read_csv(patients_path)\n",
    "        logger.info(f\"Loaded {len(patients_df)} patient records\")\n",
    "        \n",
    "        admissions_df = pd.read_csv(admissions_path)\n",
    "        logger.info(f\"Loaded {len(admissions_df)} admission records\")\n",
    "        \n",
    "        # Merge to get complete demographic information\n",
    "        demographics = pd.merge(patients_df, admissions_df, on='subject_id', how='inner')\n",
    "        logger.info(f\"Combined demographics dataset contains {len(demographics)} records\")\n",
    "        \n",
    "        return demographics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading demographic data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_mimic_diagnoses():\n",
    "    \"\"\"Load diagnoses data from MIMIC-IV\"\"\"\n",
    "    diagnoses_path = os.path.join(MIMIC_DIR, 'relevant_diagnoses.csv')\n",
    "    d_icd_path = os.path.join(MIMIC_DIR, 'd_icd_diagnoses.csv')\n",
    "    \n",
    "    try:\n",
    "        # Check if we have the relevant diagnoses file\n",
    "        if os.path.exists(diagnoses_path):\n",
    "            diagnoses_df = pd.read_csv(diagnoses_path)\n",
    "        else:\n",
    "            logger.warning(f\"Relevant diagnoses file not found: {diagnoses_path}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Load the ICD codes dictionary\n",
    "        d_icd_df = pd.read_csv(d_icd_path)\n",
    "        \n",
    "        # Merge to get diagnosis descriptions\n",
    "        merged_df = pd.merge(diagnoses_df, d_icd_df, on='icd_code', how='left')\n",
    "        logger.info(f\"Loaded and merged {len(merged_df)} diagnoses with descriptions\")\n",
    "        \n",
    "        return merged_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading diagnoses data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_mimic_lab_results():\n",
    "    \"\"\"Load lab results data from MIMIC-IV\"\"\"\n",
    "    lab_results_path = os.path.join(MIMIC_DIR, 'relevant_labevents.csv')\n",
    "    lab_sample_path = os.path.join(MIMIC_DIR, 'labevents_sample.csv')\n",
    "    d_labitems_path = os.path.join(MIMIC_DIR, 'd_labitems.csv')\n",
    "    \n",
    "    try:\n",
    "        # Check if we have the relevant lab events file\n",
    "        if os.path.exists(lab_results_path):\n",
    "            lab_df = pd.read_csv(lab_results_path)\n",
    "            logger.info(f\"Loaded {len(lab_df)} relevant lab events\")\n",
    "        elif os.path.exists(lab_sample_path):\n",
    "            lab_df = pd.read_csv(lab_sample_path)\n",
    "            logger.info(f\"Loaded {len(lab_df)} sample lab events\")\n",
    "        else:\n",
    "            logger.warning(f\"No lab events file found\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Load the lab items dictionary\n",
    "        if os.path.exists(d_labitems_path):\n",
    "            d_labitems_df = pd.read_csv(d_labitems_path)\n",
    "            \n",
    "            # Merge to get lab test descriptions\n",
    "            merged_df = pd.merge(lab_df, d_labitems_df, on='itemid', how='left')\n",
    "            logger.info(f\"Merged {len(merged_df)} lab events with descriptions\")\n",
    "            \n",
    "            return merged_df\n",
    "        else:\n",
    "            logger.warning(f\"Lab items dictionary not found: {d_labitems_path}\")\n",
    "            return lab_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading lab results data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_mimic_text_data():\n",
    "    \"\"\"Load text data from MIMIC-IV for NER\"\"\"\n",
    "    omr_path = os.path.join(MIMIC_DIR, 'omr_sample.csv')\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(omr_path):\n",
    "            omr_df = pd.read_csv(omr_path)\n",
    "            logger.info(f\"Loaded {len(omr_df)} OMR records for text analysis\")\n",
    "            \n",
    "            # Extract text columns for NER\n",
    "            text_columns = [col for col in omr_df.columns if omr_df[col].dtype == 'object']\n",
    "            text_df = omr_df[['subject_id', 'hadm_id'] + text_columns]\n",
    "            \n",
    "            return text_df\n",
    "        else:\n",
    "            logger.warning(f\"OMR data not found: {omr_path}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading text data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_sample_clinical_notes():\n",
    "    \"\"\"Get sample clinical notes for NER from MIMIC-IV\"\"\"\n",
    "    text_df = load_mimic_text_data()\n",
    "    \n",
    "    if not text_df.empty:\n",
    "        # Extract text columns and combine them\n",
    "        text_columns = [col for col in text_df.columns if col not in ['subject_id', 'hadm_id']]\n",
    "        \n",
    "        # Combine all text columns into a single note for each patient\n",
    "        notes = []\n",
    "        for _, row in text_df.iterrows():\n",
    "            note_parts = []\n",
    "            for col in text_columns:\n",
    "                if isinstance(row[col], str) and len(row[col].strip()) > 0:\n",
    "                    note_parts.append(f\"{col}: {row[col]}\")\n",
    "            \n",
    "            if note_parts:\n",
    "                note = \"\\n\".join(note_parts)\n",
    "                notes.append({\n",
    "                    'subject_id': row['subject_id'],\n",
    "                    'hadm_id': row['hadm_id'],\n",
    "                    'note_text': note\n",
    "                })\n",
    "        \n",
    "        if notes:\n",
    "            notes_df = pd.DataFrame(notes)\n",
    "            logger.info(f\"Created {len(notes_df)} clinical notes for NER\")\n",
    "            return notes_df\n",
    "    \n",
    "    logger.warning(\"No suitable clinical notes found in MIMIC-IV data\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def prepare_mimic_for_classification():\n",
    "    \"\"\"Prepare MIMIC-IV data for the classification pipeline\"\"\"\n",
    "    # Load all the necessary components\n",
    "    demographics = load_mimic_demographics()\n",
    "    diagnoses = load_mimic_diagnoses()\n",
    "    lab_results = load_mimic_lab_results()\n",
    "    \n",
    "    if demographics.empty:\n",
    "        logger.warning(\"Cannot prepare classification data without demographics\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Start with demographics as our base\n",
    "        classification_data = demographics.copy()\n",
    "        \n",
    "        # Add a COVID flag if we have diagnoses\n",
    "        if not diagnoses.empty:\n",
    "            # Find COVID-specific ICD codes\n",
    "            covid_diagnoses = diagnoses[diagnoses['long_title'].str.contains('COVID|coronavirus|SARS-CoV', case=False, na=False)]\n",
    "            covid_patients = covid_diagnoses['subject_id'].unique().tolist()\n",
    "            \n",
    "            # Add COVID flag to classification data\n",
    "            classification_data['covid_diagnosis'] = classification_data['subject_id'].isin(covid_patients)\n",
    "            \n",
    "            logger.info(f\"Identified {len(covid_patients)} patients with COVID-19 diagnoses\")\n",
    "        \n",
    "        # Add lab result features if available\n",
    "        if not lab_results.empty:\n",
    "            # Find COVID-specific lab tests\n",
    "            covid_tests = lab_results[lab_results['label'].str.contains('COVID|coronavirus|SARS', case=False, na=False)]\n",
    "            \n",
    "            if not covid_tests.empty:\n",
    "                # Create a pivot table of COVID test results\n",
    "                covid_test_pivot = covid_tests.pivot_table(\n",
    "                    index='subject_id',\n",
    "                    columns='itemid',\n",
    "                    values='valuenum',\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                \n",
    "                # Rename columns to test names\n",
    "                itemid_to_label = dict(zip(covid_tests['itemid'], covid_tests['label']))\n",
    "                covid_test_pivot.columns = [f\"test_{itemid_to_label.get(col, col)}\" for col in covid_test_pivot.columns]\n",
    "                \n",
    "                # Reset index to make subject_id a column again\n",
    "                covid_test_pivot.reset_index(inplace=True)\n",
    "                \n",
    "                # Merge with classification data\n",
    "                classification_data = pd.merge(classification_data, covid_test_pivot, on='subject_id', how='left')\n",
    "                \n",
    "                logger.info(f\"Added {len(covid_test_pivot.columns)-1} COVID test features to classification data\")\n",
    "        \n",
    "        # Save the prepared data\n",
    "        output_path = os.path.join(MIMIC_DIR, 'classification_data.csv')\n",
    "        classification_data.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved classification data with {len(classification_data)} rows and {len(classification_data.columns)} features to {output_path}\")\n",
    "        \n",
    "        return classification_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing classification data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test loading functions\n",
    "    demographics = load_mimic_demographics()\n",
    "    diagnoses = load_mimic_diagnoses()\n",
    "    lab_results = load_mimic_lab_results()\n",
    "    text_data = load_mimic_text_data()\n",
    "    \n",
    "    # Prepare data for our pipeline\n",
    "    classification_data = prepare_mimic_for_classification()\n",
    "    notes = get_sample_clinical_notes()\n",
    "    \n",
    "    print(\"\\nData Summary:\")\n",
    "    print(f\"Demographics: {len(demographics)} records\")\n",
    "    print(f\"Diagnoses: {len(diagnoses)} records\")\n",
    "    print(f\"Lab Results: {len(lab_results)} records\")\n",
    "    print(f\"Text Data: {len(text_data)} records\")\n",
    "    print(f\"Classification Data: {len(classification_data)} records with {len(classification_data.columns)} features\")\n",
    "    print(f\"Clinical Notes: {len(notes)} notes\")\n",
    "\"\"\"\n",
    "    \n",
    "    with open(module_path, 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    print(f\"Created MIMIC integration module at {module_path}\")\n",
    "\n",
    "# Create the integration module\n",
    "create_mimic_integration_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've accomplished the following:\n",
    "\n",
    "1. Explored the structure of the MIMIC-IV dataset\n",
    "2. Identified relevant tables for our COVID-19 detection project\n",
    "3. Extracted and sampled key data including:\n",
    "   - Patient demographics\n",
    "   - Diagnoses (especially respiratory and possible COVID-19 cases)\n",
    "   - Lab results\n",
    "   - Some text data from the OMR table\n",
    "4. Created a Python module to integrate this data with our pipeline\n",
    "\n",
    "### Notes on the Data\n",
    "\n",
    "1. MIMIC-IV does not include full clinical notes in the standard release. These are part of a separate module (MIMIC-IV-Note) that may require additional access.\n",
    "2. For NER tasks, we'll need to rely on the limited text data available in tables like OMR, or continue using our synthetic data approach.\n",
    "3. For the classification tasks, we have good structured data including demographics, diagnoses, and lab results.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Use the integrated MIMIC data in our NER pipeline where possible\n",
    "2. Build and train our classification model using the structured data\n",
    "3. Consider applying for access to MIMIC-IV-Note for more comprehensive clinical notes in the future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}