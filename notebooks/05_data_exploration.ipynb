{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Data Exploration\n",
    "\n",
    "This notebook explores real COVID-19 datasets for our detection pipeline.\n",
    "\n",
    "We'll examine various datasets for both pipeline stages:\n",
    "1. **Stage 1**: Unstructured text for NER (clinical notes, research papers, etc.)\n",
    "2. **Stage 2**: Structured EHR data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from src.data_fetcher import list_available_datasets\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available COVID-19 Datasets\n",
    "\n",
    "First, let's review the available datasets we can use for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CORD-19 Research Papers\n",
      "   Description: COVID-19 Open Research Dataset of scientific papers\n",
      "   Data Type: Unstructured text (research papers)\n",
      "   Access: Public\n",
      "\n",
      "2. CDC COVID-19 Case Surveillance\n",
      "   Description: De-identified patient-level data on COVID-19 cases\n",
      "   Data Type: Structured data (patient records)\n",
      "   Access: Public\n",
      "\n",
      "3. COVID-19 Clinical Trials\n",
      "   Description: Clinical trials related to COVID-19 with detailed descriptions\n",
      "   Data Type: Semi-structured text (trial descriptions)\n",
      "   Access: Public\n",
      "\n",
      "4. COVID-19 Twitter Dataset\n",
      "   Description: Tweets related to COVID-19 symptoms and experiences\n",
      "   Data Type: Unstructured text (social media)\n",
      "   Access: Public\n",
      "\n",
      "5. MIMIC-III Clinical Database\n",
      "   Description: Medical information for ICU patients (includes some COVID cases)\n",
      "   Data Type: Structured data + unstructured clinical notes\n",
      "   Access: Requires credential application\n",
      "\n",
      "6. i2b2 NLP Research Datasets\n",
      "   Description: Clinical NLP datasets with some COVID-19 related content\n",
      "   Data Type: Unstructured text (clinical notes)\n",
      "   Access: Requires application\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = list_available_datasets()\n",
    "\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"{i}. {dataset['name']}\")\n",
    "    print(f\"   Description: {dataset['description']}\")\n",
    "    print(f\"   Data Type: {dataset['data_type']}\")\n",
    "    print(f\"   Access: {dataset['access']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Unstructured Text Data for NER\n",
    "\n",
    "For our NER pipeline, we need unstructured text with descriptions of COVID-19 symptoms, treatments, and clinical presentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORD-19 Research Papers\n",
    "\n",
    "The COVID-19 Open Research Dataset (CORD-19) contains scientific papers about COVID-19 and related coronaviruses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 424824",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m cord19_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/external/cord19_metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cord19_path):\n\u001b[0;32m----> 5\u001b[0m     cord19_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcord19_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cord19_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CORD-19 papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     display(cord19_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/Documents/Disease_Prediction_Project/covid_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Disease_Prediction_Project/covid_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Disease_Prediction_Project/covid_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/Disease_Prediction_Project/covid_venv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 424824"
     ]
    }
   ],
   "source": [
    "# Load CORD-19 metadata (if available)\n",
    "cord19_path = '../data/external/cord19_metadata.csv'\n",
    "\n",
    "if os.path.exists(cord19_path):\n",
    "    cord19_df = pd.read_csv(cord19_path)\n",
    "    print(f\"Loaded {len(cord19_df)} CORD-19 papers\")\n",
    "    display(cord19_df.head())\n",
    "    \n",
    "    # Show columns\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in cord19_df.columns:\n",
    "        print(f\" - {col}\")\n",
    "else:\n",
    "    print(\"CORD-19 metadata not found. Please download it first using the data_fetcher.py module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Trials Text Data\n",
    "\n",
    "ClinicalTrials.gov provides detailed descriptions of COVID-19 trials, including symptoms, eligibility criteria, and interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical trials data not found. Please download it first using the data_fetcher.py module.\n"
     ]
    }
   ],
   "source": [
    "# Load clinical trials data (if available)\n",
    "trials_path = '../data/external/covid19_clinical_trials.json'\n",
    "\n",
    "if os.path.exists(trials_path):\n",
    "    with open(trials_path, 'r') as f:\n",
    "        trials_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(trials_data['StudyFieldsResponse']['StudyFields'])} clinical trials\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    trials_df = pd.DataFrame(trials_data['StudyFieldsResponse']['StudyFields'])\n",
    "    display(trials_df.head())\n",
    "    \n",
    "    # Sample detailed descriptions\n",
    "    if 'DetailedDescription' in trials_df.columns:\n",
    "        print(\"\\nSample Clinical Trial Description:\")\n",
    "        sample_desc = trials_df[trials_df['DetailedDescription'].apply(lambda x: len(x) > 0)]['DetailedDescription'].iloc[0][0]\n",
    "        print(sample_desc[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Clinical trials data not found. Please download it first using the data_fetcher.py module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data Analysis\n",
    "\n",
    "Twitter data provides real-world accounts of COVID-19 symptoms and experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter data (if available)\n",
    "twitter_path = '../data/external/covid19_tweets.tsv'\n",
    "\n",
    "if os.path.exists(twitter_path):\n",
    "    # Read with pandas if available\n",
    "    try:\n",
    "        twitter_df = pd.read_csv(twitter_path, sep='\\t')\n",
    "        print(f\"Loaded {len(twitter_df)} COVID-19 tweets\")\n",
    "        display(twitter_df.head())\n",
    "        \n",
    "        # Show sample tweets\n",
    "        tweet_col = [col for col in twitter_df.columns if 'text' in col.lower() or 'tweet' in col.lower()][0]\n",
    "        print(\"\\nSample tweets mentioning symptoms:\")\n",
    "        symptom_tweets = twitter_df[twitter_df[tweet_col].str.contains('symptom|cough|fever|breath', case=False, na=False)]\n",
    "        for tweet in symptom_tweets[tweet_col].head(3).tolist():\n",
    "            print(f\" - {tweet}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Twitter data: {e}\")\n",
    "else:\n",
    "    print(\"Twitter data not found. Please download it first using the data_fetcher.py module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Structured EHR Data for Classification\n",
    "\n",
    "For our classification model, we need structured patient data with COVID-19 diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC COVID-19 Case Surveillance Data\n",
    "\n",
    "This dataset contains de-identified patient data collected by state health departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 rows from CDC COVID-19 case surveillance data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cdc_case_earliest_dt</th>\n",
       "      <th>cdc_report_dt</th>\n",
       "      <th>pos_spec_dt</th>\n",
       "      <th>onset_dt</th>\n",
       "      <th>current_status</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>race_ethnicity_combined</th>\n",
       "      <th>hosp_yn</th>\n",
       "      <th>icu_yn</th>\n",
       "      <th>death_yn</th>\n",
       "      <th>medcond_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024/03/08</td>\n",
       "      <td>2024/03/08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>70 - 79 Years</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/09/12</td>\n",
       "      <td>2022/09/12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>70 - 79 Years</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/07/21</td>\n",
       "      <td>2022/07/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Laboratory-confirmed case</td>\n",
       "      <td>Male</td>\n",
       "      <td>70 - 79 Years</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/11/10</td>\n",
       "      <td>2022/11/10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Probable Case</td>\n",
       "      <td>Male</td>\n",
       "      <td>70 - 79 Years</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/08/22</td>\n",
       "      <td>2022/08/22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Probable Case</td>\n",
       "      <td>Male</td>\n",
       "      <td>70 - 79 Years</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cdc_case_earliest_dt  cdc_report_dt pos_spec_dt onset_dt  \\\n",
       "0            2024/03/08    2024/03/08         NaN      NaN   \n",
       "1            2022/09/12    2022/09/12         NaN      NaN   \n",
       "2            2022/07/21    2022/07/21         NaN      NaN   \n",
       "3            2022/11/10    2022/11/10         NaN      NaN   \n",
       "4            2022/08/22    2022/08/22         NaN      NaN   \n",
       "\n",
       "              current_status   sex      age_group race_ethnicity_combined  \\\n",
       "0  Laboratory-confirmed case  Male  70 - 79 Years                 Unknown   \n",
       "1  Laboratory-confirmed case  Male  70 - 79 Years                 Unknown   \n",
       "2  Laboratory-confirmed case  Male  70 - 79 Years                 Unknown   \n",
       "3              Probable Case  Male  70 - 79 Years                 Unknown   \n",
       "4              Probable Case  Male  70 - 79 Years                 Unknown   \n",
       "\n",
       "   hosp_yn   icu_yn death_yn medcond_yn  \n",
       "0  Missing  Missing  Unknown    Missing  \n",
       "1  Missing  Missing  Unknown    Missing  \n",
       "2  Missing  Missing  Unknown    Missing  \n",
       "3  Missing  Missing  Unknown    Missing  \n",
       "4  Missing  Missing  Unknown    Missing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns:\n",
      " - cdc_case_earliest_dt \n",
      " - cdc_report_dt\n",
      " - pos_spec_dt\n",
      " - onset_dt\n",
      " - current_status\n",
      " - sex\n",
      " - age_group\n",
      " - race_ethnicity_combined\n",
      " - hosp_yn\n",
      " - icu_yn\n",
      " - death_yn\n",
      " - medcond_yn\n",
      "\n",
      "Case status distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "current_status\n",
       "Laboratory-confirmed case    8159\n",
       "Probable Case                1841\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load CDC case surveillance data (if available)\n",
    "cdc_path = '../data/external/covid19_case_surveillance.csv'\n",
    "\n",
    "if os.path.exists(cdc_path):\n",
    "    # Read just the first 10000 rows to avoid memory issues\n",
    "    cdc_df = pd.read_csv(cdc_path, nrows=10000)\n",
    "    print(f\"Loaded 10000 rows from CDC COVID-19 case surveillance data\")\n",
    "    display(cdc_df.head())\n",
    "    \n",
    "    # Column info\n",
    "    print(\"\\nColumns:\")\n",
    "    for col in cdc_df.columns:\n",
    "        print(f\" - {col}\")\n",
    "        \n",
    "    # Basic statistics\n",
    "    if 'current_status' in cdc_df.columns:\n",
    "        print(\"\\nCase status distribution:\")\n",
    "        display(cdc_df['current_status'].value_counts())\n",
    "else:\n",
    "    print(\"CDC data not found. Please download it first using the data_fetcher.py module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-III Clinical Notes\n",
    "\n",
    "MIMIC contains real clinical notes, but requires credentialed access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC data not found. Note that MIMIC requires credentialed access.\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC notes (if available)\n",
    "mimic_path = '../data/external/mimic_notes.csv'\n",
    "\n",
    "if os.path.exists(mimic_path):\n",
    "    # Read just the first 1000 rows to avoid memory issues\n",
    "    mimic_df = pd.read_csv(mimic_path, nrows=1000)\n",
    "    print(f\"Loaded 1000 rows from MIMIC clinical notes\")\n",
    "    display(mimic_df.head())\n",
    "    \n",
    "    # Note types\n",
    "    if 'CATEGORY' in mimic_df.columns:\n",
    "        print(\"\\nNote categories:\")\n",
    "        display(mimic_df['CATEGORY'].value_counts())\n",
    "        \n",
    "    # Sample text\n",
    "    if 'TEXT' in mimic_df.columns:\n",
    "        print(\"\\nSample clinical note:\")\n",
    "        sample_note = mimic_df['TEXT'].iloc[0]\n",
    "        print(sample_note[:500] + \"...\")\n",
    "else:\n",
    "    print(\"MIMIC data not found. Note that MIMIC requires credentialed access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Strategy\n",
    "\n",
    "Based on our exploration, here's how we can combine these datasets for our pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: NER Data Integration\n",
    "\n",
    "For the NER stage, we need to extract medical entities from unstructured text. Here's our strategy:\n",
    "\n",
    "1. **Primary source**: CORD-19 abstracts and clinical trials descriptions\n",
    "   - Rich medical terminology and symptom descriptions\n",
    "   - Formal medical language similar to clinical notes\n",
    "\n",
    "2. **Secondary source**: Twitter data\n",
    "   - Real-world symptom descriptions in lay language\n",
    "   - Useful for understanding how patients describe symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of combining text sources for NER training\n",
    "def prepare_ner_corpus(cord19_path=None, trials_path=None, twitter_path=None):\n",
    "    \"\"\"\n",
    "    Prepare a corpus of texts for NER training.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    \n",
    "    # Add CORD-19 abstracts if available\n",
    "    if cord19_path and os.path.exists(cord19_path):\n",
    "        cord19_df = pd.read_csv(cord19_path)\n",
    "        if 'abstract' in cord19_df.columns:\n",
    "            abstracts = cord19_df['abstract'].dropna().tolist()\n",
    "            corpus.extend(abstracts[:100])  # Limit to 100 for demonstration\n",
    "            print(f\"Added {len(abstracts[:100])} CORD-19 abstracts\")\n",
    "    \n",
    "    # Add clinical trial descriptions if available\n",
    "    if trials_path and os.path.exists(trials_path):\n",
    "        with open(trials_path, 'r') as f:\n",
    "            trials_data = json.load(f)\n",
    "        \n",
    "        trials_df = pd.DataFrame(trials_data['StudyFieldsResponse']['StudyFields'])\n",
    "        if 'DetailedDescription' in trials_df.columns:\n",
    "            descriptions = [desc[0] for desc in trials_df['DetailedDescription'] if desc]\n",
    "            corpus.extend(descriptions[:50])  # Limit to 50 for demonstration\n",
    "            print(f\"Added {len(descriptions[:50])} clinical trial descriptions\")\n",
    "    \n",
    "    # Add tweets if available\n",
    "    if twitter_path and os.path.exists(twitter_path):\n",
    "        twitter_df = pd.read_csv(twitter_path, sep='\\t')\n",
    "        tweet_col = [col for col in twitter_df.columns if 'text' in col.lower() or 'tweet' in col.lower()][0]\n",
    "        symptom_tweets = twitter_df[twitter_df[tweet_col].str.contains('symptom|cough|fever|breath', case=False, na=False)]\n",
    "        tweets = symptom_tweets[tweet_col].dropna().tolist()\n",
    "        corpus.extend(tweets[:200])  # Limit to 200 for demonstration\n",
    "        print(f\"Added {len(tweets[:200])} COVID-related tweets\")\n",
    "    \n",
    "    print(f\"\\nTotal corpus size: {len(corpus)} documents\")\n",
    "    return corpus\n",
    "\n",
    "# Don't run this yet - we'll wait until we have the data\n",
    "# ner_corpus = prepare_ner_corpus(\n",
    "#     cord19_path='../data/external/cord19_metadata.csv',\n",
    "#     trials_path='../data/external/covid19_clinical_trials.json',\n",
    "#     twitter_path='../data/external/covid19_tweets.tsv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Classification Data Integration\n",
    "\n",
    "For the classification stage, we need structured patient data with COVID-19 diagnoses. Here's our strategy:\n",
    "\n",
    "1. **Primary source**: CDC Case Surveillance data\n",
    "   - Contains demographic information and COVID-19 test results\n",
    "   - Large sample size for training classification models\n",
    "\n",
    "2. **Secondary source**: Extracted features from NER\n",
    "   - Will add extracted symptoms and severity as features\n",
    "   - Bridges the gap between unstructured text and structured prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preparing classification data\n",
    "def prepare_classification_data(cdc_path=None, mimic_path=None):\n",
    "    \"\"\"\n",
    "    Prepare structured data for COVID-19 classification.\n",
    "    \"\"\"\n",
    "    classification_data = None\n",
    "    \n",
    "    # Use CDC data if available\n",
    "    if cdc_path and os.path.exists(cdc_path):\n",
    "        cdc_df = pd.read_csv(cdc_path, nrows=10000)  # Limit rows for demonstration\n",
    "        \n",
    "        # Select relevant columns and clean\n",
    "        relevant_cols = [col for col in cdc_df.columns if col in [\n",
    "            'current_status', 'sex', 'age_group', 'race', 'ethnicity',\n",
    "            'hosp_yn', 'icu_yn', 'death_yn', 'medcond_yn'\n",
    "        ]]\n",
    "        \n",
    "        if relevant_cols:\n",
    "            classification_data = cdc_df[relevant_cols].copy()\n",
    "            print(f\"Prepared classification data with {len(classification_data)} rows and {len(relevant_cols)} features\")\n",
    "        else:\n",
    "            print(\"No relevant columns found in CDC data\")\n",
    "    \n",
    "    # Use MIMIC data if available\n",
    "    if mimic_path and os.path.exists(mimic_path) and classification_data is None:\n",
    "        # This would require custom processing for MIMIC\n",
    "        print(\"MIMIC data processing would be implemented here\")\n",
    "    \n",
    "    if classification_data is None:\n",
    "        print(\"No classification data available\")\n",
    "        return None\n",
    "        \n",
    "    return classification_data\n",
    "\n",
    "# Don't run this yet - we'll wait until we have the data\n",
    "# classification_data = prepare_classification_data(\n",
    "#     cdc_path='../data/external/covid19_case_surveillance.csv',\n",
    "#     mimic_path='../data/external/mimic_notes.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on our exploration, here are the next steps for our COVID-19 detection pipeline:\n",
    "\n",
    "1. **Download the datasets**\n",
    "   - CORD-19 metadata and papers\n",
    "   - CDC case surveillance data\n",
    "   - Clinical trials data\n",
    "   - Twitter data (if available)\n",
    "\n",
    "2. **Prepare NER training data**\n",
    "   - Extract and clean text from multiple sources\n",
    "   - Annotate a sample for training our NER models\n",
    "\n",
    "3. **Prepare classification features**\n",
    "   - Process structured CDC data\n",
    "   - Prepare to integrate extracted entities from NER stage\n",
    "\n",
    "4. **Implement the complete pipeline**\n",
    "   - NER to extract medical entities\n",
    "   - Classification to predict COVID-19 likelihood\n",
    "\n",
    "The next notebook will focus on preparing the NER training data from these sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
