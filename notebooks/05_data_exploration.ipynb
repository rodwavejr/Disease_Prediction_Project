{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# COVID-19 Data Exploration\n\nThis notebook explores COVID-19 datasets for our detection pipeline. It supports both synthetic and real data modes.\n\nWe'll examine various datasets for both pipeline stages:\n1. **Stage 1**: Unstructured text for NER (clinical notes, research papers, etc.)\n2. **Stage 2**: Structured EHR data for classification\n\n## Data Source Configuration\n\nThis notebook will work with either:\n- **Synthetic data**: Generated by our data_collection.py module (default during development)\n- **Real data**: Downloaded from actual sources using download_real_data.py\n\nTo switch modes, set `USE_SYNTHETIC_DATA = False` in src/data_collection.py or use the `--real` flag with scripts.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom datetime import datetime\n\n# Add project root to path\nsys.path.append('..')\nfrom src.data_fetcher import list_available_datasets\nfrom src.data_collection import USE_SYNTHETIC_DATA\n\n# Configure display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 20)\n\n# Display data source\nprint(f\"Using {'SYNTHETIC' if USE_SYNTHETIC_DATA else 'REAL'} data\")\nprint(\"To switch between synthetic and real data, update USE_SYNTHETIC_DATA in src/data_collection.py\")\nprint(\"or run: python download_real_data.py --datasets all\")\nprint(\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available COVID-19 Datasets\n",
    "\n",
    "First, let's review the available datasets we can use for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. CORD-19 Research Papers\n",
      "   Description: COVID-19 Open Research Dataset of scientific papers\n",
      "   Data Type: Unstructured text (research papers)\n",
      "   Access: Public\n",
      "\n",
      "2. CDC COVID-19 Case Surveillance\n",
      "   Description: De-identified patient-level data on COVID-19 cases\n",
      "   Data Type: Structured data (patient records)\n",
      "   Access: Public\n",
      "\n",
      "3. COVID-19 Clinical Trials\n",
      "   Description: Clinical trials related to COVID-19 with detailed descriptions\n",
      "   Data Type: Semi-structured text (trial descriptions)\n",
      "   Access: Public\n",
      "\n",
      "4. COVID-19 Twitter Dataset\n",
      "   Description: Tweets related to COVID-19 symptoms and experiences\n",
      "   Data Type: Unstructured text (social media)\n",
      "   Access: Public\n",
      "\n",
      "5. MIMIC-III Clinical Database\n",
      "   Description: Medical information for ICU patients (includes some COVID cases)\n",
      "   Data Type: Structured data + unstructured clinical notes\n",
      "   Access: Requires credential application\n",
      "\n",
      "6. i2b2 NLP Research Datasets\n",
      "   Description: Clinical NLP datasets with some COVID-19 related content\n",
      "   Data Type: Unstructured text (clinical notes)\n",
      "   Access: Requires application\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = list_available_datasets()\n",
    "\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"{i}. {dataset['name']}\")\n",
    "    print(f\"   Description: {dataset['description']}\")\n",
    "    print(f\"   Data Type: {dataset['data_type']}\")\n",
    "    print(f\"   Access: {dataset['access']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Unstructured Text Data for NER\n",
    "\n",
    "For our NER pipeline, we need unstructured text with descriptions of COVID-19 symptoms, treatments, and clinical presentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORD-19 Research Papers\n",
    "\n",
    "The COVID-19 Open Research Dataset (CORD-19) contains scientific papers about COVID-19 and related coronaviruses."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load CORD-19 metadata (if available)\ncord19_path = '../data/external/cord19_metadata.csv'\n\nif os.path.exists(cord19_path):\n    try:\n        # Try to read with error handling for malformed CSV\n        try:\n            # For newer pandas versions\n            cord19_df = pd.read_csv(cord19_path, on_bad_lines='skip')\n        except TypeError:\n            # For older pandas versions\n            cord19_df = pd.read_csv(cord19_path, error_bad_lines=False)\n            \n        print(f\"Loaded {len(cord19_df)} CORD-19 papers\")\n        display(cord19_df.head())\n        \n        # Show columns\n        print(\"\\nColumns:\")\n        for col in cord19_df.columns:\n            print(f\" - {col}\")\n    except Exception as e:\n        print(f\"Error reading CORD-19 metadata: {e}\")\n        print(\"Try running the download_real_data.py script first to fix CSV issues\")\nelse:\n    print(\"CORD-19 metadata not found. Please download it by running:\")\n    print(\"python download_real_data.py --datasets cord19\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Trials Text Data\n",
    "\n",
    "ClinicalTrials.gov provides detailed descriptions of COVID-19 trials, including symptoms, eligibility criteria, and interventions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load clinical trials data (if available)\ntrials_path = '../data/external/covid19_clinical_trials.json'\n\nif os.path.exists(trials_path):\n    try:\n        with open(trials_path, 'r') as f:\n            trials_data = json.load(f)\n        \n        print(f\"Loaded {len(trials_data['StudyFieldsResponse']['StudyFields'])} clinical trials\")\n        \n        # Convert to DataFrame\n        trials_df = pd.DataFrame(trials_data['StudyFieldsResponse']['StudyFields'])\n        display(trials_df.head())\n        \n        # Sample detailed descriptions\n        if 'DetailedDescription' in trials_df.columns:\n            print(\"\\nSample Clinical Trial Description:\")\n            sample_desc = trials_df[trials_df['DetailedDescription'].apply(lambda x: len(x) > 0)]['DetailedDescription'].iloc[0][0]\n            print(sample_desc[:500] + \"...\")\n    except Exception as e:\n        print(f\"Error processing clinical trials data: {e}\")\nelse:\n    print(\"Clinical trials data not found. Please download it by running:\")\n    print(\"python download_real_data.py --datasets trials\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data Analysis\n",
    "\n",
    "Twitter data provides real-world accounts of COVID-19 symptoms and experiences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load Twitter data (if available)\ntwitter_path = '../data/external/covid19_tweets.tsv'\n\nif os.path.exists(twitter_path):\n    # Read with pandas if available\n    try:\n        twitter_df = pd.read_csv(twitter_path, sep='\\t', error_bad_lines=False if pd.__version__ < '1.3.0' else None,\n                               on_bad_lines='skip' if pd.__version__ >= '1.3.0' else None)\n        print(f\"Loaded {len(twitter_df)} COVID-19 tweets\")\n        display(twitter_df.head())\n        \n        # Show sample tweets\n        tweet_col = [col for col in twitter_df.columns if 'text' in col.lower() or 'tweet' in col.lower()]\n        if tweet_col:\n            tweet_col = tweet_col[0]\n            print(\"\\nSample tweets mentioning symptoms:\")\n            symptom_tweets = twitter_df[twitter_df[tweet_col].str.contains('symptom|cough|fever|breath', case=False, na=False)]\n            for tweet in symptom_tweets[tweet_col].head(3).tolist():\n                print(f\" - {tweet}\")\n        else:\n            print(\"No text column found in Twitter data\")\n    except Exception as e:\n        print(f\"Error reading Twitter data: {e}\")\nelse:\n    print(\"Twitter data not found. Please download it by running:\")\n    print(\"python download_real_data.py --datasets twitter\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Structured EHR Data for Classification\n",
    "\n",
    "For our classification model, we need structured patient data with COVID-19 diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC COVID-19 Case Surveillance Data\n",
    "\n",
    "This dataset contains de-identified patient data collected by state health departments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load CDC case surveillance data (if available)\ncdc_path = '../data/external/covid19_case_surveillance.csv'\n\nif os.path.exists(cdc_path):\n    # Read just the first 10000 rows to avoid memory issues\n    try:\n        try:\n            # For newer pandas versions\n            cdc_df = pd.read_csv(cdc_path, nrows=10000, on_bad_lines='skip')\n        except TypeError:\n            # For older pandas versions\n            cdc_df = pd.read_csv(cdc_path, nrows=10000, error_bad_lines=False)\n            \n        print(f\"Loaded 10000 rows from CDC COVID-19 case surveillance data\")\n        display(cdc_df.head())\n        \n        # Column info\n        print(\"\\nColumns:\")\n        for col in cdc_df.columns:\n            print(f\" - {col}\")\n            \n        # Basic statistics\n        if 'current_status' in cdc_df.columns:\n            print(\"\\nCase status distribution:\")\n            display(cdc_df['current_status'].value_counts())\n    except Exception as e:\n        print(f\"Error reading CDC data: {e}\")\nelse:\n    print(\"CDC data not found. Please download it by running:\")\n    print(\"python download_real_data.py --datasets cdc\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-III Clinical Notes\n",
    "\n",
    "MIMIC contains real clinical notes, but requires credentialed access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC data not found. Note that MIMIC requires credentialed access.\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC notes (if available)\n",
    "mimic_path = '../data/external/mimic_notes.csv'\n",
    "\n",
    "if os.path.exists(mimic_path):\n",
    "    # Read just the first 1000 rows to avoid memory issues\n",
    "    mimic_df = pd.read_csv(mimic_path, nrows=1000)\n",
    "    print(f\"Loaded 1000 rows from MIMIC clinical notes\")\n",
    "    display(mimic_df.head())\n",
    "    \n",
    "    # Note types\n",
    "    if 'CATEGORY' in mimic_df.columns:\n",
    "        print(\"\\nNote categories:\")\n",
    "        display(mimic_df['CATEGORY'].value_counts())\n",
    "        \n",
    "    # Sample text\n",
    "    if 'TEXT' in mimic_df.columns:\n",
    "        print(\"\\nSample clinical note:\")\n",
    "        sample_note = mimic_df['TEXT'].iloc[0]\n",
    "        print(sample_note[:500] + \"...\")\n",
    "else:\n",
    "    print(\"MIMIC data not found. Note that MIMIC requires credentialed access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration Strategy\n",
    "\n",
    "Based on our exploration, here's how we can combine these datasets for our pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: NER Data Integration\n",
    "\n",
    "For the NER stage, we need to extract medical entities from unstructured text. Here's our strategy:\n",
    "\n",
    "1. **Primary source**: CORD-19 abstracts and clinical trials descriptions\n",
    "   - Rich medical terminology and symptom descriptions\n",
    "   - Formal medical language similar to clinical notes\n",
    "\n",
    "2. **Secondary source**: Twitter data\n",
    "   - Real-world symptom descriptions in lay language\n",
    "   - Useful for understanding how patients describe symptoms"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example of combining text sources for NER training\ndef prepare_ner_corpus(cord19_path=None, trials_path=None, twitter_path=None):\n    \"\"\"\n    Prepare a corpus of texts for NER training.\n    \"\"\"\n    corpus = []\n    \n    # Add CORD-19 abstracts if available\n    if cord19_path and os.path.exists(cord19_path):\n        try:\n            # Handle potential CSV issues\n            try:\n                # For newer pandas versions\n                cord19_df = pd.read_csv(cord19_path, on_bad_lines='skip')\n            except TypeError:\n                # For older pandas versions\n                cord19_df = pd.read_csv(cord19_path, error_bad_lines=False)\n                \n            if 'abstract' in cord19_df.columns:\n                abstracts = cord19_df['abstract'].dropna().tolist()\n                corpus.extend(abstracts[:100])  # Limit to 100 for demonstration\n                print(f\"Added {len(abstracts[:100])} CORD-19 abstracts\")\n            else:\n                print(\"No 'abstract' column found in CORD-19 data\")\n        except Exception as e:\n            print(f\"Error processing CORD-19 data: {e}\")\n    \n    # Add clinical trial descriptions if available\n    if trials_path and os.path.exists(trials_path):\n        try:\n            with open(trials_path, 'r') as f:\n                trials_data = json.load(f)\n            \n            trials_df = pd.DataFrame(trials_data['StudyFieldsResponse']['StudyFields'])\n            if 'DetailedDescription' in trials_df.columns:\n                descriptions = [desc[0] for desc in trials_df['DetailedDescription'] if desc and len(desc) > 0]\n                corpus.extend(descriptions[:50])  # Limit to 50 for demonstration\n                print(f\"Added {len(descriptions[:50])} clinical trial descriptions\")\n            else:\n                print(\"No 'DetailedDescription' column found in clinical trials data\")\n        except Exception as e:\n            print(f\"Error processing clinical trials data: {e}\")\n    \n    # Add tweets if available\n    if twitter_path and os.path.exists(twitter_path):\n        try:\n            # Handle potential TSV issues\n            try:\n                # For newer pandas versions\n                twitter_df = pd.read_csv(twitter_path, sep='\\t', on_bad_lines='skip')\n            except TypeError:\n                # For older pandas versions\n                twitter_df = pd.read_csv(twitter_path, sep='\\t', error_bad_lines=False)\n                \n            tweet_col = [col for col in twitter_df.columns if 'text' in col.lower() or 'tweet' in col.lower()]\n            if tweet_col:\n                tweet_col = tweet_col[0]\n                symptom_tweets = twitter_df[twitter_df[tweet_col].str.contains('symptom|cough|fever|breath', case=False, na=False)]\n                tweets = symptom_tweets[tweet_col].dropna().tolist()\n                corpus.extend(tweets[:200])  # Limit to 200 for demonstration\n                print(f\"Added {len(tweets[:200])} COVID-related tweets\")\n            else:\n                print(\"No text column found in Twitter data\")\n        except Exception as e:\n            print(f\"Error processing Twitter data: {e}\")\n    \n    print(f\"\\nTotal corpus size: {len(corpus)} documents\")\n    return corpus\n\n# Don't run this yet - we'll wait until we have the data\n# ner_corpus = prepare_ner_corpus(\n#     cord19_path='../data/external/cord19_metadata.csv',\n#     trials_path='../data/external/covid19_clinical_trials.json',\n#     twitter_path='../data/external/covid19_tweets.tsv'\n# )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Classification Data Integration\n",
    "\n",
    "For the classification stage, we need structured patient data with COVID-19 diagnoses. Here's our strategy:\n",
    "\n",
    "1. **Primary source**: CDC Case Surveillance data\n",
    "   - Contains demographic information and COVID-19 test results\n",
    "   - Large sample size for training classification models\n",
    "\n",
    "2. **Secondary source**: Extracted features from NER\n",
    "   - Will add extracted symptoms and severity as features\n",
    "   - Bridges the gap between unstructured text and structured prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preparing classification data\n",
    "def prepare_classification_data(cdc_path=None, mimic_path=None):\n",
    "    \"\"\"\n",
    "    Prepare structured data for COVID-19 classification.\n",
    "    \"\"\"\n",
    "    classification_data = None\n",
    "    \n",
    "    # Use CDC data if available\n",
    "    if cdc_path and os.path.exists(cdc_path):\n",
    "        cdc_df = pd.read_csv(cdc_path, nrows=10000)  # Limit rows for demonstration\n",
    "        \n",
    "        # Select relevant columns and clean\n",
    "        relevant_cols = [col for col in cdc_df.columns if col in [\n",
    "            'current_status', 'sex', 'age_group', 'race', 'ethnicity',\n",
    "            'hosp_yn', 'icu_yn', 'death_yn', 'medcond_yn'\n",
    "        ]]\n",
    "        \n",
    "        if relevant_cols:\n",
    "            classification_data = cdc_df[relevant_cols].copy()\n",
    "            print(f\"Prepared classification data with {len(classification_data)} rows and {len(relevant_cols)} features\")\n",
    "        else:\n",
    "            print(\"No relevant columns found in CDC data\")\n",
    "    \n",
    "    # Use MIMIC data if available\n",
    "    if mimic_path and os.path.exists(mimic_path) and classification_data is None:\n",
    "        # This would require custom processing for MIMIC\n",
    "        print(\"MIMIC data processing would be implemented here\")\n",
    "    \n",
    "    if classification_data is None:\n",
    "        print(\"No classification data available\")\n",
    "        return None\n",
    "        \n",
    "    return classification_data\n",
    "\n",
    "# Don't run this yet - we'll wait until we have the data\n",
    "# classification_data = prepare_classification_data(\n",
    "#     cdc_path='../data/external/covid19_case_surveillance.csv',\n",
    "#     mimic_path='../data/external/mimic_notes.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Next Steps\n\nBased on our exploration, here are the next steps for our COVID-19 detection pipeline:\n\n1. **Download the datasets**\n   - Run `python download_real_data.py --datasets all` to download all available datasets\n   - This will download and process:\n       - CORD-19 metadata and papers\n       - CDC case surveillance data\n       - Clinical trials data\n       - Twitter data (if available)\n   - For datasets requiring credentials (MIMIC, i2b2), follow application procedures\n\n2. **Prepare NER training data**\n   - Extract and clean text from multiple sources using the `prepare_ner_corpus` function\n   - Annotate a sample for training our NER models\n   - Use both rule-based and ML-based NER approaches\n\n3. **Prepare classification features**\n   - Process structured CDC data\n   - Integrate extracted entities from NER stage\n\n4. **Implement the complete pipeline**\n   - NER to extract medical entities \n   - Classification to predict COVID-19 likelihood\n\n5. **Toggle between synthetic and real data**\n   - Edit `src/data_collection.py` to set `USE_SYNTHETIC_DATA = False` when ready\n   - Or run scripts with the `--real` flag:\n     ```\n     python src/data_collection.py --real\n     ```\n\nThe next notebook will focus on preparing the NER training data from these sources.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}